{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import EfficientNet_B0_Weights\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "from PIL import Image\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMER BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Block.\n",
    "\n",
    "    Implements a single Transformer Encoder Block as proposed in \"Attention Is All You Need\" (Vaswani et al.).\n",
    "\n",
    "    This block consists of:\n",
    "    - Multi-Head Self-Attention\n",
    "    - Feedforward Neural Network (FFN)\n",
    "    - Residual connections and Layer Normalization\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimension of the input embeddings/features.\n",
    "        num_heads (int): Number of attention heads. Default is 8.\n",
    "        ff_hidden_dim (int): Dimension of the hidden layer in the feedforward network. Default is 2048.\n",
    "        dropout (float): Dropout rate applied after attention and feedforward layers. Default is 0.1.\n",
    "\n",
    "    Example:\n",
    "        >>> encoder_block = TransformerEncoderBlock(input_dim=512, num_heads=8)\n",
    "        >>> x = torch.randn(32, 10, 512)  # (batch_size, sequence_length, input_dim)\n",
    "        >>> output = encoder_block(x)\n",
    "\n",
    "    Shape:\n",
    "        - Input: (B, L, C) where B = batch size, L = sequence length, and C = input_dim.\n",
    "        - Output: (B, L, C)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_heads=8, ff_hidden_dim=2048, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "\n",
    "        # Multi-Head Self-Attention\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Layer Normalization for attention output\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # Feedforward Network (FFN)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(input_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "        # Layer Normalization for FFN output\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer Encoder Block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, L, C).\n",
    "            mask (torch.Tensor, optional): Attention mask of shape (B, L) or (L, L).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (B, L, C).\n",
    "        \"\"\"\n",
    "        # Multi-Head Self-Attention with residual connection\n",
    "        attn_output, _ = self.self_attention(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feedforward Network with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIDEO CLASSIFY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassify(nn.Module):\n",
    "    def __init__(self, num_classes=10, frames=16):\n",
    "        super(VideoClassify, self).__init__()\n",
    "        \n",
    "        # Use EfficientNet B0\n",
    "        self.backbone = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove the classifier from the backbone\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        \n",
    "        # EfficientNet B0's last feature map has 1280 output channels\n",
    "        \n",
    "        # Direct transformation from backbone output to 128 dimensions with stronger downsampling\n",
    "        self.feature_reducer = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((2, 2)),  # Aggressive spatial downsampling to 2x2\n",
    "            nn.Conv2d(1280, 128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Flatten(),  # Flatten from [B, 128, 2, 2] to [B, 512]\n",
    "            nn.Linear(512, 128)  # Add linear layer to get to 128 dimensions\n",
    "        )\n",
    "\n",
    "        # Keep the rest of the architecture similar, but with small dimensions\n",
    "        self.pos_embedding = LearnablePositionalEmbedding(128, max_len=frames)\n",
    "        \n",
    "        # Smaller transformer to save memory\n",
    "        self.transformer_block = TransformerEncoderBlock(input_dim=128, num_heads=4, ff_hidden_dim=128, dropout=0.1)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.frames = frames\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input is in the shape of (B, T, 3, 224, 224) where B is batch size and T is number of frames\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "        \n",
    "        # Reshape to process all frames as a batch\n",
    "        x = x.reshape(-1, 3, 224, 224)  # (B*T, 3, 224, 224)\n",
    "        \n",
    "        # Extract features from backbone\n",
    "        features = self.backbone(x)  # Get the feature map\n",
    "        \n",
    "        # Reduce dimensions\n",
    "        spatial_features = self.feature_reducer(features)  # (B*T, 128)\n",
    "        \n",
    "        # Reshape to separate batch and time dimensions\n",
    "        spatial_features = spatial_features.reshape(batch_size, seq_len, 128)\n",
    "        \n",
    "        # Add learnable positional embeddings for temporal information\n",
    "        spatial_features = self.pos_embedding(spatial_features)\n",
    "        \n",
    "        # Apply transformer for temporal modeling\n",
    "        temporal_features = self.transformer_block(spatial_features)  # (B, T, 128)\n",
    "        \n",
    "        # Global average pooling across time dimension\n",
    "        pooled_features = torch.mean(temporal_features, dim=1)  # (B, 128)\n",
    "   \n",
    "        # Classification\n",
    "        output = self.classifier(pooled_features)  # (B, num_classes)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class LearnablePositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable positional embeddings for each position in the sequence.\n",
    "    Each frame position gets its own learnable embedding vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super(LearnablePositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create learnable parameter for position embeddings\n",
    "        # Shape: (1, max_len, d_model) - one embedding vector per position\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        \n",
    "        # Initialize the position embeddings\n",
    "        nn.init.trunc_normal_(self.position_embeddings, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]        \"\"\"\n",
    "        # Add position embeddings to input features\n",
    "        # Each frame at position i gets the same learnable embedding vector\n",
    "        x = x + self.position_embeddings[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET/DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading videos from a directory structure.\n",
    "    \n",
    "    The dataset expects videos to be organized in folders where each folder represents\n",
    "    a class. It samples frames at even intervals to match the required number of frames\n",
    "    for the VideoClassify model.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Root directory containing video folders.\n",
    "        num_frames (int): Number of frames to sample from each video.\n",
    "        transform (callable, optional): Optional transform to be applied on sampled frames.\n",
    "        extensions (list): List of valid video file extensions. Default: ['.mp4', '.avi', '.mov']\n",
    "        class_map (dict, optional): Dictionary mapping folder names to class indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, num_frames=16, transform=None, \n",
    "                 extensions=['.mp4', '.avi', '.mov', '.MOV'], class_map=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.extensions = extensions\n",
    "        \n",
    "        # Default transform if none provided\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "        # Find all video files and their corresponding classes\n",
    "        self.video_paths = []\n",
    "        self.video_labels = []\n",
    "        \n",
    "        # Get class folders\n",
    "        class_folders = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        \n",
    "        # Create class map if not provided\n",
    "        if class_map is None:\n",
    "            self.class_map = {folder: idx for idx, folder in enumerate(sorted(class_folders))}\n",
    "        else:\n",
    "            self.class_map = class_map\n",
    "        \n",
    "        # Collect videos and their labels\n",
    "        for class_folder in class_folders:\n",
    "            if class_folder in self.class_map:\n",
    "                class_path = os.path.join(root_dir, class_folder)\n",
    "                class_label = self.class_map[class_folder]\n",
    "                \n",
    "                # Find video files with specified extensions\n",
    "                for ext in self.extensions:\n",
    "                    videos = glob.glob(os.path.join(class_path, f'*{ext}'))\n",
    "                    for video_path in videos:\n",
    "                        self.video_paths.append(video_path)\n",
    "                        self.video_labels.append(class_label)\n",
    "        \n",
    "        print(f\"Found {len(self.video_paths)} videos across {len(class_folders)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def sample_frames(self, video_path):\n",
    "        \"\"\"\n",
    "        Sample frames at even intervals from a video.\n",
    "        \n",
    "        Args:\n",
    "            video_path (str): Path to the video file.\n",
    "            \n",
    "        Returns:\n",
    "            list: List of sampled frames as PIL Images.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video: {video_path}\")\n",
    "        \n",
    "        # Get video properties\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0:\n",
    "            raise ValueError(f\"Video has no frames: {video_path}\")\n",
    "        \n",
    "        # Calculate frame indices to sample\n",
    "        if total_frames <= self.num_frames:\n",
    "            # If video has fewer frames than needed, duplicate frames\n",
    "            indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "        else:\n",
    "            # Sample frames at even intervals\n",
    "            indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "        \n",
    "        frames = []\n",
    "        for idx in indices:\n",
    "            # Set frame position\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                # If frame read failed, create a black frame\n",
    "                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Convert BGR (OpenCV) to RGB (PIL)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get video frames and label for a given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the video to get.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (frames, label) where frames is a tensor of shape (num_frames, 3, H, W)\n",
    "            and label is the class index.\n",
    "        \"\"\"\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.video_labels[idx]\n",
    "        \n",
    "        # Sample frames from the video\n",
    "        frames = self.sample_frames(video_path)\n",
    "        \n",
    "        # Apply transforms to each frame\n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "        \n",
    "        # Stack frames to create a tensor of shape (T, C, H, W)\n",
    "        frames_tensor = torch.stack(frames)\n",
    "        \n",
    "        return frames_tensor, label\n",
    "\n",
    "# Example usage to create data loaders\n",
    "def create_data_loaders(root_dir, batch_size=8, num_frames=16, num_workers=4, \n",
    "                       train_transform=None, val_transform=None, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Create train and validation data loaders for video classification.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Root directory containing video folders\n",
    "        batch_size (int): Batch size for data loaders\n",
    "        num_frames (int): Number of frames to sample per video\n",
    "        num_workers (int): Number of workers for data loading\n",
    "        train_transform (callable, optional): Transform for training data\n",
    "        val_transform (callable, optional): Transform for validation data\n",
    "        train_ratio (float): Ratio of training data (0.0 to 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader)\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader, random_split\n",
    "    \n",
    "    # Default transforms\n",
    "    if train_transform is None:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    if val_transform is None:\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    # Create full dataset\n",
    "    full_dataset = VideoDataset(root_dir=root_dir, num_frames=num_frames, transform=None)\n",
    "    \n",
    "    # Split dataset into train and validation\n",
    "    train_size = int(train_ratio * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create dataset objects with appropriate transforms\n",
    "    train_dataset = VideoDataset(\n",
    "        root_dir=root_dir,\n",
    "        num_frames=num_frames,\n",
    "        transform=train_transform,\n",
    "        class_map=full_dataset.class_map,\n",
    "    )\n",
    "    \n",
    "    val_dataset = VideoDataset(\n",
    "        root_dir=root_dir,\n",
    "        num_frames=num_frames,\n",
    "        transform=val_transform,\n",
    "        class_map=full_dataset.class_map,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN/VALIDATE FUNCTIONS AND CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'data_dir': 'dataset',           # Path to dataset directory\n",
    "    'output_dir': './checkpoints',     # Path to save checkpoints and logs\n",
    "    'frames': 75,                      # Number of frames to sample per video\n",
    "    'batch_size': 1,                   # Training batch size\n",
    "    'val_batch_size': 1,              # Validation batch size\n",
    "    'epochs': 50,                      # Number of training epochs\n",
    "    'lr': 0.001,                       # Initial learning rate\n",
    "    'weight_decay': 1e-4,              # Weight decay\n",
    "    'num_workers': 0,                  # Number of data loading workers\n",
    "    'save_freq': 5                     # Save checkpoint every N epochs\n",
    "}\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "    \n",
    "    # Set up device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader = create_data_loaders(\n",
    "        root_dir=CONFIG['data_dir'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        num_frames=CONFIG['frames'],\n",
    "        num_workers=CONFIG['num_workers']\n",
    "    )\n",
    "    \n",
    "    # Get number of classes from dataset\n",
    "    num_classes = len(train_loader.dataset.class_map)\n",
    "    print(f\"Training on {num_classes} classes\")\n",
    "    \n",
    "    # Create model\n",
    "    model = VideoClassify(num_classes=num_classes, frames=CONFIG['frames'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    \n",
    "    # Set up loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Training state\n",
    "    start_epoch = 0\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, CONFIG['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model=model,\n",
    "            data_loader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            epoch=epoch\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(\n",
    "            model=model,\n",
    "            data_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        is_best = val_acc > best_val_acc\n",
    "        best_val_acc = max(val_acc, best_val_acc)\n",
    "        \n",
    "        if is_best or (epoch + 1) % CONFIG['save_freq'] == 0:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "            }, is_best)\n",
    "    \n",
    "    print(f\"Training completed. Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for inputs, labels in pbar:\n",
    "        # Move inputs and labels to device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "        processed_size += inputs.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss / processed_size,\n",
    "            'acc': running_corrects / processed_size,\n",
    "            'time': f\"{time.time() - start_time:.2f}s\"\n",
    "        })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    epoch_acc = running_corrects / len(data_loader.dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    \"\"\"Evaluate the model on the validation set.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=\"Validating\"):\n",
    "            # Move inputs and labels to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data).item()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    epoch_acc = running_corrects / len(data_loader.dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth'):\n",
    "    \"\"\"Save training checkpoint.\"\"\"\n",
    "    checkpoint_path = os.path.join(CONFIG['output_dir'], filename)\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    if is_best:\n",
    "        best_path = os.path.join(CONFIG['output_dir'], 'best_model.pth')\n",
    "        torch.save(state, best_path)\n",
    "        print(f\"Best model saved to {best_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTRY POINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 57 videos across 5 classes\n",
      "Found 57 videos across 5 classes\n",
      "Found 57 videos across 5 classes\n",
      "Training on 5 classes\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  74%|███████▎  | 42/57 [15:18<05:28, 21.87s/it, loss=1.48, acc=0.381, time=905.64s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m train_loss, train_acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     67\u001b[39m val_loss, val_acc = validate(\n\u001b[32m     68\u001b[39m     model=model,\n\u001b[32m     69\u001b[39m     data_loader=val_loader,\n\u001b[32m     70\u001b[39m     criterion=criterion,\n\u001b[32m     71\u001b[39m     device=device\n\u001b[32m     72\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, data_loader, criterion, optimizer, device, epoch)\u001b[39m\n\u001b[32m    108\u001b[39m start_time = time.time()\n\u001b[32m    110\u001b[39m pbar = tqdm(data_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move inputs and labels to device\u001b[39;49;00m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NOVA\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NOVA\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NOVA\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NOVA\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mVideoDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    122\u001b[39m label = \u001b[38;5;28mself\u001b[39m.video_labels[idx]\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Sample frames from the video\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m frames = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Apply transforms to each frame\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mVideoDataset.sample_frames\u001b[39m\u001b[34m(self, video_path)\u001b[39m\n\u001b[32m     90\u001b[39m frames = []\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[32m     92\u001b[39m     \u001b[38;5;66;03m# Set frame position\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCAP_PROP_POS_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     ret, frame = cap.read()\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     97\u001b[39m         \u001b[38;5;66;03m# If frame read failed, create a black frame\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
