{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from typing import List, Tuple, Dict, Optional, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET/DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Dataset for loading video data for camera technique classification.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        video_paths: List[str],\n",
    "        labels: List[int],\n",
    "        num_frames: int = 64,\n",
    "        transform=None,\n",
    "        temporal_sample_method: str = \"uniform\",\n",
    "        frame_size: Tuple[int, int] = (224, 224),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_paths: List of paths to video files\n",
    "            labels: List of class labels for each video\n",
    "            num_frames: Number of frames to extract from each video\n",
    "            transform: Optional transforms to apply to frames\n",
    "            temporal_sample_method: Method to sample frames (\"uniform\" or \"random\")\n",
    "            frame_size: Size to resize frames to (height, width)\n",
    "        \"\"\"\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.temporal_sample_method = temporal_sample_method\n",
    "        self.frame_size = frame_size\n",
    "        \n",
    "        # Default transform if none provided\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def _load_video(self, video_path: str) -> np.ndarray:\n",
    "        \"\"\"Load video and extract frames.\"\"\"\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        # Get total frame count\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames <= 0:\n",
    "            raise ValueError(f\"Failed to load video: {video_path}\")\n",
    "        \n",
    "        # Choose frame indices based on sampling method\n",
    "        if self.temporal_sample_method == \"uniform\":\n",
    "            # Uniformly sample frames across the video\n",
    "            indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "        elif self.temporal_sample_method == \"random\":\n",
    "            # Randomly sample frames\n",
    "            indices = sorted(random.sample(range(total_frames), min(self.num_frames, total_frames)))\n",
    "            # If we need more frames than the video has, we'll cycle through\n",
    "            if len(indices) < self.num_frames:\n",
    "                extra = np.random.choice(indices, self.num_frames - len(indices))\n",
    "                indices = np.concatenate([indices, extra])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported temporal sampling method: {self.temporal_sample_method}\")\n",
    "        \n",
    "        # Extract selected frames\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                # If frame reading fails, create a black frame\n",
    "                frame = np.zeros((self.frame_size[0], self.frame_size[1], 3), dtype=np.uint8)\n",
    "            else:\n",
    "                # Convert from BGR to RGB\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # Resize frame\n",
    "                frame = cv2.resize(frame, (self.frame_size[1], self.frame_size[0]))\n",
    "            \n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        return np.array(frames)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Get video frames and label for a given index.\"\"\"\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load video frames\n",
    "        frames = self._load_video(video_path)\n",
    "        \n",
    "        # Apply transforms to each frame\n",
    "        transformed_frames = []\n",
    "        for frame in frames:\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            transformed_frames.append(frame)\n",
    "        \n",
    "        # Stack frames along time dimension\n",
    "        # Output shape: (T, C, H, W)\n",
    "        video_tensor = torch.stack(transformed_frames)\n",
    "        \n",
    "        # Rearrange to expected model input shape: (T, C, H, W)\n",
    "        return video_tensor, label\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    data_dir: str,\n",
    "    batch_size: int = 8,\n",
    "    num_frames: int = 64,\n",
    "    num_workers: int = 4,\n",
    "    train_ratio: float = 0.8,\n",
    "    frame_size: Tuple[int, int] = (224, 224),\n",
    "    video_extensions: Union[List[str], str] = \"*.mp4\",\n",
    ") -> Tuple[DataLoader, DataLoader, Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Create train and validation dataloaders from a directory of videos.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing class folders with videos\n",
    "        batch_size: Batch size for dataloaders\n",
    "        num_frames: Number of frames to sample from each video\n",
    "        num_workers: Number of worker processes for dataloaders\n",
    "        train_ratio: Ratio of data to use for training\n",
    "        frame_size: Size to resize frames to (height, width)\n",
    "        video_extensions: File extensions to look for (e.g., \"*.mp4\" or [\"*.mp4\", \"*.avi\"])\n",
    "        \n",
    "    Returns:\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        class_to_idx: Dictionary mapping class indices to class names\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Check if data directory exists\n",
    "    if not data_dir.exists():\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    # Get class folders\n",
    "    class_dirs = [d for d in data_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if len(class_dirs) == 0:\n",
    "        raise ValueError(f\"No class directories found in {data_dir}. Please make sure your data is organized in subdirectories, with each subdirectory representing a class.\")\n",
    "    \n",
    "    print(f\"Found {len(class_dirs)} class directories: {[d.name for d in class_dirs]}\")\n",
    "    \n",
    "    class_to_idx = {cls.name: i for i, cls in enumerate(class_dirs)}\n",
    "    \n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Handle both string and list of extensions\n",
    "    if isinstance(video_extensions, str):\n",
    "        video_extensions = [video_extensions]\n",
    "    \n",
    "    # Collect video paths and labels\n",
    "    for class_dir in class_dirs:\n",
    "        class_idx = class_to_idx[class_dir.name]\n",
    "        class_videos = []\n",
    "        \n",
    "        # Try multiple extensions\n",
    "        for ext in video_extensions:\n",
    "            class_videos.extend(list(class_dir.glob(ext)))\n",
    "        \n",
    "        if not class_videos:\n",
    "            print(f\"Warning: No videos found in {class_dir} with extensions {video_extensions}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found {len(class_videos)} videos in class '{class_dir.name}'\")\n",
    "        \n",
    "        for video_file in class_videos:\n",
    "            video_paths.append(str(video_file))\n",
    "            labels.append(class_idx)\n",
    "    \n",
    "    if len(video_paths) == 0:\n",
    "        raise ValueError(f\"No video files found in the class directories. \"\n",
    "                         f\"Checked extensions: {video_extensions}. \"\n",
    "                         f\"Please make sure your videos have the correct extensions.\")\n",
    "    \n",
    "    print(f\"Total videos found: {len(video_paths)}\")\n",
    "    \n",
    "    # Create train/val split\n",
    "    indices = list(range(len(video_paths)))\n",
    "    random.shuffle(indices)\n",
    "    split = int(train_ratio * len(indices))\n",
    "    \n",
    "    if split == 0:\n",
    "        split = 1  # Ensure at least one sample in training set\n",
    "    \n",
    "    train_indices = indices[:split]\n",
    "    val_indices = indices[split:] if split < len(indices) else [indices[0]]  # Ensure at least one sample in validation\n",
    "    \n",
    "    print(f\"Training samples: {len(train_indices)}, Validation samples: {len(val_indices)}\")\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(frame_size[0]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(frame_size[0] + 32),\n",
    "        transforms.CenterCrop(frame_size[0]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = VideoDataset(\n",
    "        [video_paths[i] for i in train_indices],\n",
    "        [labels[i] for i in train_indices],\n",
    "        num_frames=num_frames,\n",
    "        transform=train_transform,\n",
    "        temporal_sample_method=\"random\",\n",
    "        frame_size=frame_size\n",
    "    )\n",
    "    \n",
    "    val_dataset = VideoDataset(\n",
    "        [video_paths[i] for i in val_indices],\n",
    "        [labels[i] for i in val_indices],\n",
    "        num_frames=num_frames,\n",
    "        transform=val_transform,\n",
    "        temporal_sample_method=\"uniform\",\n",
    "        frame_size=frame_size\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=min(batch_size, len(train_dataset)),  # Ensure batch size isn't larger than dataset\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True if len(train_dataset) > batch_size else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=min(batch_size, len(val_dataset)),  # Ensure batch size isn't larger than dataset\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    # Invert class_to_idx for easier interpretation\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    \n",
    "    return train_loader, val_loader, idx_to_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnablePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100, spatial=True):\n",
    "        super(LearnablePositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.spatial = spatial  # Determines if it’s used for spatial or temporal encoding\n",
    "        \n",
    "        # Different shapes for spatial vs. temporal embeddings\n",
    "        if self.spatial:\n",
    "            self.position_embeddings = nn.Parameter(torch.zeros(1, max_len, 1, d_model))  # (1, 196, 1, D)\n",
    "        else:\n",
    "            self.position_embeddings = nn.Parameter(torch.zeros(1, 1, max_len, d_model))  # (1, 1, 16, D)\n",
    "        \n",
    "        nn.init.trunc_normal_(self.position_embeddings, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 196, T, D) for temporal encoding or (B, T, 196, D) for spatial encoding\n",
    "        \"\"\"\n",
    "        x = x + self.position_embeddings[:, :x.size(1), :, :] if self.spatial else x + self.position_embeddings[:, :, :x.size(2), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Block.\n",
    "\n",
    "    Implements a single Transformer Encoder Block as proposed in \"Attention Is All You Need\" (Vaswani et al.).\n",
    "\n",
    "    This block consists of:\n",
    "    - Multi-Head Self-Attention\n",
    "    - Feedforward Neural Network (FFN)\n",
    "    - Residual connections and Layer Normalization\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimension of the input embeddings/features.\n",
    "        num_heads (int): Number of attention heads. Default is 8.\n",
    "        ff_hidden_dim (int): Dimension of the hidden layer in the feedforward network. Default is 2048.\n",
    "        dropout (float): Dropout rate applied after attention and feedforward layers. Default is 0.1.\n",
    "\n",
    "    Example:\n",
    "        >>> encoder_block = TransformerEncoderBlock(input_dim=512, num_heads=8)\n",
    "        >>> x = torch.randn(32, 10, 512)  # (batch_size, sequence_length, input_dim)\n",
    "        >>> output = encoder_block(x)\n",
    "\n",
    "    Shape:\n",
    "        - Input: (B, L, C) where B = batch size, L = sequence length, and C = input_dim.\n",
    "        - Output: (B, L, C)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_heads=8, ff_hidden_dim=2048, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "\n",
    "        # Multi-Head Self-Attention\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Layer Normalization for attention output\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # Feedforward Network (FFN)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(input_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "        # Layer Normalization for FFN output\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer Encoder Block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, L, C).\n",
    "            mask (torch.Tensor, optional): Attention mask of shape (B, L) or (L, L).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (B, L, C).\n",
    "        \"\"\"\n",
    "        # Multi-Head Self-Attention with residual connection\n",
    "        attn_output, _ = self.self_attention(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feedforward Network with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "- INPUT: (B, T, 3, 224, 224)\n",
    "- PATCH: (B, T, P, D) where P=224*224/(16*16)=196 patches and D=768 embedding dimension\n",
    "- CLS: Add CLS token to get (B, T, P+1, D) before applying positional encodings\n",
    "- SPATIAL: Apply spatial positional encoding, then permute to (B, P+1, T, D) for temporal encoding\n",
    "- TEMPORAL: Apply temporal encoding, then reshape to (B*T, P+1, D) for spatial attention\n",
    "- After spatial attention, split CLS tokens and reshape patches to (B, P, T, D) for temporal attention\n",
    "\"\"\"\n",
    "\n",
    "class TimeSFormer(nn.Module):\n",
    "    def __init__(self, number_of_frames: int, num_classes: int = 10):\n",
    "        super(TimeSFormer, self).__init__()\n",
    "\n",
    "        self._INPUT_DIM = 224\n",
    "        self._INPUT_CHANNEL = 3\n",
    "        self._PATCH_SIZE = 16\n",
    "        self._EMBED_DIM = 768\n",
    "        self._PATCH_NUM = (224 // self._PATCH_SIZE) ** 2  # 196\n",
    "        self._FRAME_NUM = number_of_frames\n",
    "        \n",
    "        \n",
    "        # Positional encoding. The +1 in `self._PATCH_NUM + 1` and `self._FRAME_NUM + 1` is for CLS Token\n",
    "        self.spatial_encoding = LearnablePositionalEmbedding(self._EMBED_DIM, 0.15, self._PATCH_NUM + 1, spatial=True)\n",
    "        self.temporal_encoding = LearnablePositionalEmbedding(self._EMBED_DIM, 0.15, self._FRAME_NUM + 1, spatial=False)\n",
    "\n",
    "        # Patch embedding layer (convert patches to embeddings)\n",
    "        self.patch_embedding = nn.Linear(self._PATCH_SIZE * self._PATCH_SIZE * 3, self._EMBED_DIM)\n",
    "\n",
    "        \n",
    "        # Attention\n",
    "        self.spartial_encoder = TransformerEncoderBlock(self._EMBED_DIM, 8, 1024, 0.1)\n",
    "        self.temporal_encoder = TransformerEncoderBlock(self._EMBED_DIM, 16, 2048, 0.1)\n",
    "        \n",
    "        \n",
    "        #CLS token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self._EMBED_DIM))\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self._EMBED_DIM, num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # X has the shape of (B, T, 3, 224, 224)\n",
    "        B, T, C, H, W = x.shape\n",
    "\n",
    "        if T != self._FRAME_NUM:\n",
    "            raise ValueError(f\"The number of frames in input is {T}, mismatch with the model's expected number of frames {self._FRAME_NUM}\")\n",
    "\n",
    "        if  (C != self._INPUT_CHANNEL) or (H != self._INPUT_DIM) or (W != self._INPUT_DIM):\n",
    "            raise ValueError(f\"The input spartial dimension is ({C}, {H}, {W}), mismatch with the model's expected spartial dimension ({self._INPUT_CHANNEL}, {self._INPUT_DIM}, {self._INPUT_DIM})\")\n",
    "        \n",
    "        # Extract patches using unfold (B, T, C, H, W) -> (B, T, P, 16*16*3)\n",
    "        x = x.reshape(B * T, C, H, W)  # Flatten batch & time\n",
    "        x = x.unfold(2, self._PATCH_SIZE, self._PATCH_SIZE).unfold(3, self._PATCH_SIZE, self._PATCH_SIZE)\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()  # (B*T, 14, 14, 3, 16, 16)\n",
    "        x = x.reshape(B, T, self._PATCH_NUM, -1)  # (B, T, P, D)\n",
    "\n",
    "        # Apply patch embedding\n",
    "        patch_embeds = self.patch_embedding(x)  # (B, T, P, D)\n",
    "        \n",
    "        # Reshape to add CLS token\n",
    "        patch_embeds = patch_embeds.reshape(B*T, self._PATCH_NUM, -1)  # (B*T, P, D)\n",
    "        \n",
    "        # Add CLS tokens for each frame in the batch\n",
    "        cls_tokens = self.cls_token.expand(B*T, -1, -1)  # (B*T, 1, D)\n",
    "        tokens = torch.cat([cls_tokens, patch_embeds], dim=1)  # (B*T, P+1, D)\n",
    "        \n",
    "        # Reshape back to include time dimension\n",
    "        tokens = tokens.reshape(B, T, self._PATCH_NUM + 1, -1)  # (B, T, P+1, D)\n",
    "        \n",
    "        # Add spatial encoding\n",
    "        tokens = self.spatial_encoding(tokens)  # (B, T, P+1, D)\n",
    "        \n",
    "        # Swap and add temporal encoding\n",
    "        tokens = tokens.permute(0, 2, 1, 3)  # (B, P+1, T, D)\n",
    "        tokens = self.temporal_encoding(tokens)  # (B, P+1, T, D)\n",
    "        \n",
    "        # Restore to (B, T, P+1, D) shape\n",
    "        tokens = tokens.permute(0, 2, 1, 3)  # (B, T, P+1, D)\n",
    "        \n",
    "        # Reshape for spatial attention\n",
    "        tokens = tokens.reshape(B*T, self._PATCH_NUM + 1, -1)  # (B*T, P+1, D)\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        spartial_atten = self.spartial_encoder(tokens)  # (B*T, P+1, D)\n",
    "        \n",
    "        # Split CLS tokens and patch embeddings\n",
    "        cls_token_after_spartial = spartial_atten[:, 0:1, :]  # (B*T, 1, D)\n",
    "        patch_embeds = spartial_atten[:, 1:, :]  # (B*T, P, D)\n",
    "        \n",
    "        # Reshape for temporal attention\n",
    "        patch_embeds = patch_embeds.reshape(B, T, self._PATCH_NUM, -1)  # (B, T, P, D)\n",
    "        patch_embeds = patch_embeds.permute(0, 2, 1, 3)  # (B, P, T, D)\n",
    "        \n",
    "        # Apply temporal attention on each patch position\n",
    "        patch_embeds = patch_embeds.reshape(B*self._PATCH_NUM, T, -1)  # (B*P, T, D)\n",
    "        patch_embeds = self.temporal_encoder(patch_embeds)  # (B*P, T, D)\n",
    "        patch_embeds = patch_embeds.reshape(B, self._PATCH_NUM, T, -1)  # (B, P, T, D)\n",
    "        \n",
    "        # Process the CLS token for classification\n",
    "        # We'll use the spatially-attended CLS token (before temporal)\n",
    "        cls_token_after_spartial = cls_token_after_spartial.reshape(B, T, self._EMBED_DIM)  # (B, T, D)\n",
    "        \n",
    "        # Average the CLS token over time dimension\n",
    "        global_cls = cls_token_after_spartial.mean(dim=1)  # (B, D)\n",
    "        \n",
    "        # Apply classification head\n",
    "        output = self.classifier(self.dropout(global_cls))  # (B, num_classes)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN/VALIDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary\n",
    "CONFIG = {\n",
    "    # Data parameters\n",
    "    \"data_dir\": \"dataset\",  # Directory containing video data\n",
    "    \"output_dir\": \"output\",  # Output directory for logs and checkpoints\n",
    "    \"num_frames\": 64,  # Number of frames to sample from each video\n",
    "    \"train_ratio\": 0.8,  # Ratio of data to use for training\n",
    "    \"video_extensions\": [\"*.mp4\", \"*.avi\", \"*.mov\", \"*.mkv\", \"*.MOV\"],  # Video file extensions to look for\n",
    "    \n",
    "    # Training parameters\n",
    "    \"batch_size\": 4,  # Training batch size\n",
    "    \"epochs\": 30,  # Number of training epochs\n",
    "    \"learning_rate\": 1e-4,  # Initial learning rate\n",
    "    \"min_lr\": 1e-6,  # Minimum learning rate\n",
    "    \"weight_decay\": 1e-4,  # Weight decay coefficient\n",
    "    \"save_every\": 5,  # Save checkpoint every N epochs\n",
    "    \n",
    "    # Other parameters\n",
    "    \"seed\": 42,  # Random seed\n",
    "    \"num_workers\": 0,  # Number of data loading workers\n",
    "    \"no_cuda\": False,  # Disable CUDA\n",
    "}\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1} [Train]')\n",
    "    \n",
    "    for videos, labels in progress_bar:\n",
    "        # Move data to device\n",
    "        videos = videos.to(device)  # Expects shape (B, T, C, H, W)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': running_loss / (progress_bar.n + 1),\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "    \n",
    "    train_loss = running_loss / len(dataloader)\n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model on the validation set.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc='Validation')\n",
    "        \n",
    "        for videos, labels in progress_bar:\n",
    "            # Move data to device\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Track metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            # Save predictions and targets for metric computation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': running_loss / (progress_bar.n + 1)})\n",
    "    \n",
    "    # Compute metrics\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_acc = accuracy_score(all_targets, all_predictions) * 100\n",
    "    precision = precision_score(all_targets, all_predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_targets, all_predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, metrics, save_dir):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    \n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(checkpoint, os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pth'))\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    # Set up device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and not config[\"no_cuda\"] else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(config[\"seed\"])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(config[\"seed\"])\n",
    "    \n",
    "    # Create output directories\n",
    "    output_dir = Path(config[\"output_dir\"])\n",
    "    checkpoint_dir = output_dir / 'checkpoints'\n",
    "    log_dir = output_dir / 'logs'\n",
    "    \n",
    "    for directory in [output_dir, checkpoint_dir, log_dir]:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if data directory exists\n",
    "    data_dir = Path(config[\"data_dir\"])\n",
    "    if not data_dir.exists():\n",
    "        print(f\"Error: Data directory '{data_dir}' does not exist.\")\n",
    "        print(f\"Current working directory: {Path.cwd()}\")\n",
    "        print(f\"Please create the directory or update the CONFIG['data_dir'] value.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    try:\n",
    "        # Create data loaders - add back the video_extensions parameter\n",
    "        train_loader, val_loader, idx_to_class = create_dataloader(\n",
    "            config[\"data_dir\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            num_frames=config[\"num_frames\"],\n",
    "            num_workers=config[\"num_workers\"],\n",
    "            train_ratio=config[\"train_ratio\"],\n",
    "            frame_size=(224, 224),\n",
    "            video_extensions=config[\"video_extensions\"]  # Pass the video extensions from CONFIG\n",
    "        )\n",
    "        \n",
    "        # Save class mapping\n",
    "        with open(output_dir / 'class_mapping.json', 'w') as f:\n",
    "            json.dump(idx_to_class, f, indent=4)\n",
    "        \n",
    "        print(f\"Number of training batches: {len(train_loader)}\")\n",
    "        print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "        print(f\"Class mapping: {idx_to_class}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        num_classes = len(idx_to_class)\n",
    "        model = TimeSFormer(number_of_frames=config[\"num_frames\"], num_classes=num_classes)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=config[\"epochs\"], eta_min=config[\"min_lr\"])\n",
    "        \n",
    "        # Track best model\n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            # Train for one epoch\n",
    "            train_loss, train_acc = train_one_epoch(\n",
    "                model, train_loader, criterion, optimizer, device, epoch\n",
    "            )\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = validate(model, val_loader, criterion, device)\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Log metrics\n",
    "            writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "            writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "            writer.add_scalar('Loss/val', val_metrics['val_loss'], epoch)\n",
    "            writer.add_scalar('Accuracy/val', val_metrics['val_acc'], epoch)\n",
    "            writer.add_scalar('Precision/val', val_metrics['precision'], epoch)\n",
    "            writer.add_scalar('Recall/val', val_metrics['recall'], epoch)\n",
    "            writer.add_scalar('F1/val', val_metrics['f1'], epoch)\n",
    "            writer.add_scalar('LearningRate', scheduler.get_last_lr()[0], epoch)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Epoch {epoch+1}/{config['epochs']}:\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss: {val_metrics['val_loss']:.4f}, Val Acc: {val_metrics['val_acc']:.2f}%\")\n",
    "            print(f\"  Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "            \n",
    "            # Save checkpoint if it's the best model so far\n",
    "            if val_metrics['val_acc'] > best_val_acc:\n",
    "                best_val_acc = val_metrics['val_acc']\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, scheduler, epoch, val_metrics, \n",
    "                    os.path.join(checkpoint_dir, 'best_model')\n",
    "                )\n",
    "                print(f\"  New best model saved with val acc: {best_val_acc:.2f}%\")\n",
    "            \n",
    "            # Regularly save checkpoints\n",
    "            if (epoch + 1) % config[\"save_every\"] == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, scheduler, epoch, val_metrics, checkpoint_dir\n",
    "                )\n",
    "        \n",
    "        # Save final model\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, config[\"epochs\"] - 1, val_metrics, checkpoint_dir\n",
    "        )\n",
    "        \n",
    "        writer.close()\n",
    "        print(f\"Training completed. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    main(CONFIG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
